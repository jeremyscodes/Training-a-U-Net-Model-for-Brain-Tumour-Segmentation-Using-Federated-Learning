{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference example for trained 2D U-Net model on BraTS.\n",
    "Takes a trained model and performs inference on a few validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 23:11:46) [MSC v.1916 64 bit (AMD64)]\n",
      "Windows-10-10.0.19041-SP0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import os\n",
    "\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"{}\".format(platform.platform()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using Tensorflow version 2.9.1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_pywrap_util_port' from 'tensorflow.python' (c:\\Users\\jerem\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19368/3203425480.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m        \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Intel-optimizations (DNNL) enabled:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsMklEnabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtest_intel_tensorflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19368/3203425480.py\u001b[0m in \u001b[0;36mtest_intel_tensorflow\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmajor_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmajor_version\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m        \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_util_port\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m        \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Intel-optimizations (DNNL) enabled:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pywrap_util_port\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsMklEnabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_pywrap_util_port' from 'tensorflow.python' (c:\\Users\\jerem\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py)"
     ]
    }
   ],
   "source": [
    "def test_intel_tensorflow():\n",
    "    \"\"\"\n",
    "    Check if Intel version of TensorFlow is installed\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    print(\"We are using Tensorflow version {}\".format(tf.__version__))\n",
    "           \n",
    "    major_version = int(tf.__version__.split(\".\")[0])\n",
    "    if major_version >= 2:\n",
    "       from tensorflow.python import _pywrap_util_port\n",
    "       print(\"Intel-optimizations (DNNL) enabled:\", _pywrap_util_port.IsMklEnabled())\n",
    "    else:\n",
    "       print(\"Intel-optimizations (DNNL) enabled:\", tf.pywrap_tensorflow.IsMklEnabled()) \n",
    "\n",
    "test_intel_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\output\\2d_unet_decathlon\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "saved_model_dir = os.path.join('.', 'output', '2d_unet_decathlon')\n",
    "print(saved_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for images\n",
    "png_directory = \"inference_examples\"\n",
    "if not os.path.exists(png_directory):\n",
    "    os.makedirs(png_directory)\n",
    "    \n",
    "model_filename = os.path.join(saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the DICE coefficient and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sørensen–Dice coefficient is a statistic used for comparing the similarity of two samples. Given two sets, X and Y, it is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "dice = \\frac{2|X\\cap Y|}{|X|+|Y|}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_dice(target, prediction, smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorensen Dice coefficient\n",
    "    \"\"\"\n",
    "    prediction = np.round(prediction)\n",
    "\n",
    "    numerator = 2.0 * np.sum(target * prediction) + smooth\n",
    "    denominator = np.sum(target) + np.sum(prediction) + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return coef\n",
    "\n",
    "def calc_soft_dice(target, prediction, smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorensen (Soft) Dice coefficient - Don't round predictions\n",
    "    \"\"\"\n",
    "    numerator = 2.0 * np.sum(target * prediction) + smooth\n",
    "    denominator = np.sum(target) + np.sum(prediction) + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferencing in this example can be done in 3 simple steps:\n",
    "1. Load the data\n",
    "1. Load the Keras model \n",
    "1. Perform a `model.predict` on an input image (or set of images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\Task01_BrainTumour\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join('..', 'Task01_BrainTumour')\n",
    "print(data_path)\n",
    "\n",
    "\n",
    "crop_dim=128  # Original resolution (240)\n",
    "batch_size = 128\n",
    "seed=816\n",
    "train_test_split=0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "==============================\n",
      "Dataset name:         BRATS\n",
      "Dataset description:  Gliomas segmentation tumour and oedema in on brain images\n",
      "Tensor image size:    4D\n",
      "Dataset release:      2.0 04/05/2018\n",
      "Dataset reference:    https://www.med.upenn.edu/sbia/brats2017.html\n",
      "Dataset license:      CC-BY-SA 4.0\n",
      "==============================\n",
      "******************************\n",
      "Using previous sets\n",
      "Number of training files   = 290\n",
      "Number of validation files = 97\n",
      "Number of testing files    = 97\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DatasetGenerator, get_decathlon_filelist\n",
    "\n",
    "trainFiles, validateFiles, testFiles = get_decathlon_filelist(data_path=data_path, seed=seed)\n",
    "# save trainFiles to a .txt\n",
    "file = open('testFiles.txt','w')\n",
    "for item in testFiles:\n",
    "    file.write(item+'\\n')\n",
    "file.close()\n",
    "# These are the correct test files\n",
    "\n",
    "# TODO: Fill in the parameters for the dataset generator to return the `testing` data\n",
    "ds_test = DatasetGenerator(testFiles, \n",
    "                           batch_size=batch_size, \n",
    "                           crop_dim=[crop_dim, crop_dim], \n",
    "                           augment=False, \n",
    "                           seed=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data format = channels_last\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Op type not registered 'DisableCopyOnRead' in binary running on LAPTOP-BO1MLQ6H. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19368/3712762807.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mload_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperimental_io_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/job:localhost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\jerem\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jerem\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    913\u001b[0m                         ckpt_options, options, filters)\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         raise FileNotFoundError(\n\u001b[0m\u001b[0;32m    916\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n You may be trying to load on a different device \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;34m\"from the computational device. Consider setting the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Op type not registered 'DisableCopyOnRead' in binary running on LAPTOP-BO1MLQ6H. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "from model import unet\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "\n",
    "model = K.models.load_model(saved_model_dir, compile=False, custom_objects=unet().custom_objects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Perform prediction on some images. \n",
    "The prediction results will be saved in the output directory for images, which is defined by the `png_directory` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "def plot_results(ds):\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    img, msk = next(ds.ds)\n",
    "\n",
    "    idx = np.argmax(np.sum(np.sum(msk[:,:,:,0], axis=1), axis=1)) # find the slice with the largest tumor\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img[idx, :, :, 0], cmap=\"bone\", origin=\"lower\")\n",
    "    plt.title(\"MRI {}\".format(idx), fontsize=20)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(msk[idx, :, :], cmap=\"bone\", origin=\"lower\")\n",
    "    plt.title(\"Ground truth\", fontsize=20)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "\n",
    "    print(\"Index {}: \".format(idx), end=\"\")\n",
    "    \n",
    "    # Predict using the TensorFlow model\n",
    "    start_time = time.time()\n",
    "    prediction = model.predict(img[[idx]])\n",
    "    print(\"Elapsed time = {:.4f} msecs, \".format(1000.0*(time.time()-start_time)), end=\"\")\n",
    "    \n",
    "    plt.imshow(prediction[0,:,:,0], cmap=\"bone\", origin=\"lower\")\n",
    "    dice_coef = calc_dice(msk[idx], prediction)\n",
    "    print(\"Dice coefficient = {:.4f}, \".format(dice_coef), end=\"\")\n",
    "    plt.title(\"Prediction\\nDice = {:.4f}\".format(dice_coef), fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_results(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we perform inference even faster? Hmm.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out. Move on the the next tutorial section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX-License-Identifier: EPL-2.0*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) 2019-2020 Intel Corporation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
